{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual data cleaning and preProcessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we must clean and pre process our data before fitting it to model ... must for NLP(Natural Language Processing) or Data science.\n",
    "so we will do with out favourate language PYTHON. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this we will learn \n",
    "Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use  Natural Language Tool Kit (nltk), so lets install and import it in out program and most important download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1-Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For tokenization, nltk has a method word_tokenize(). \n",
    "It will break down the raw text and return as a list.\n",
    "You can also check the difference of future cleaning using the size of the token list.\n",
    "\n",
    "The first thing we must do to our data is to tokenize it. That is, break up paragraphs into sentences (sentence tokenization) or break up sentences into single words (word tokenization). \n",
    "\n",
    "For sentence tokenization, use the sent_tokenize method. Most tweets are only one sentence long, so there may not be much of a difference in the tokenization here.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets try it with some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text=\"YOU ARE usually do it in a CORPUS to break down the text into words,symbols, sentences, paragraphs, and other meaningful elements. It must be done for the future clearing of the text. Without it you can not properly clean the text like punctuation, stop words e.t.c.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list = nltk.word_tokenize(raw_text) # making a var or pointer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converted to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YOU',\n",
       " 'ARE',\n",
       " 'usually',\n",
       " 'do',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'CORPUS',\n",
       " 'to',\n",
       " 'break',\n",
       " 'down',\n",
       " 'the',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words',\n",
       " ',',\n",
       " 'symbols',\n",
       " ',',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'paragraphs',\n",
       " ',',\n",
       " 'and',\n",
       " 'other',\n",
       " 'meaningful',\n",
       " 'elements',\n",
       " '.',\n",
       " 'It',\n",
       " 'must',\n",
       " 'be',\n",
       " 'done',\n",
       " 'for',\n",
       " 'the',\n",
       " 'future',\n",
       " 'clearing',\n",
       " 'of',\n",
       " 'the',\n",
       " 'text',\n",
       " '.',\n",
       " 'Without',\n",
       " 'it',\n",
       " 'you',\n",
       " 'can',\n",
       " 'not',\n",
       " 'properly',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'text',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " ',',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'e.t.c',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YOU', 'ARE', 'usually', 'do', 'it']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see total amount of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens :  55\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens : \", len(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#now we can lower case all upper case words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step-2 all upper case are converted to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list2=[]\n",
    "for word in token_list:\n",
    "    token_list2.append(word.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'do',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'corpus',\n",
       " 'to',\n",
       " 'break',\n",
       " 'down',\n",
       " 'the',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words',\n",
       " ',',\n",
       " 'symbols',\n",
       " ',',\n",
       " 'sentences',\n",
       " ',',\n",
       " 'paragraphs',\n",
       " ',',\n",
       " 'and',\n",
       " 'other',\n",
       " 'meaningful',\n",
       " 'elements',\n",
       " '.',\n",
       " 'it',\n",
       " 'must',\n",
       " 'be',\n",
       " 'done',\n",
       " 'for',\n",
       " 'the',\n",
       " 'future',\n",
       " 'clearing',\n",
       " 'of',\n",
       " 'the',\n",
       " 'text',\n",
       " '.',\n",
       " 'without',\n",
       " 'it',\n",
       " 'you',\n",
       " 'can',\n",
       " 'not',\n",
       " 'properly',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'text',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " ',',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'e.t.c',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens :  55\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens : \", len(token_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step-3 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'are', 'usually', 'do', 'it', 'in', 'a', 'corpus', 'to', 'break', 'down', 'the', 'text', 'into', 'words', 'symbols', 'sentences', 'paragraphs', 'and', 'other'] \n",
      "\n",
      "Total tokens :  47\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import punkt\n",
    "token_list3 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list2))\n",
    "print(token_list3[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##another method by me "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "token_list3=[]\n",
    "for t in token_list2:\n",
    "    if t not in punctuations:\n",
    "        token_list3.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " 'are',\n",
       " 'usually',\n",
       " 'do',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'corpus',\n",
       " 'to',\n",
       " 'break',\n",
       " 'down',\n",
       " 'the',\n",
       " 'text',\n",
       " 'into',\n",
       " 'words',\n",
       " 'symbols',\n",
       " 'sentences',\n",
       " 'paragraphs',\n",
       " 'and',\n",
       " 'other',\n",
       " 'meaningful',\n",
       " 'elements',\n",
       " 'it',\n",
       " 'must',\n",
       " 'be',\n",
       " 'done',\n",
       " 'for',\n",
       " 'the',\n",
       " 'future',\n",
       " 'clearing',\n",
       " 'of',\n",
       " 'the',\n",
       " 'text',\n",
       " 'without',\n",
       " 'it',\n",
       " 'you',\n",
       " 'can',\n",
       " 'not',\n",
       " 'properly',\n",
       " 'clean',\n",
       " 'the',\n",
       " 'text',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'e.t.c']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens :  47\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens : \", len(token_list3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step-4 Removing StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Stop words does not contribute to the text analysis as they don’t have any meaning. Example of the stop words are like in, the, and which e.t.c. It’s better that you should remove from them. Nltk has already the list of the stop words you can use them to compare your tokenize words. You can download the stop words using nltk.download(“stopwords”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WordListCorpusReader in 'C:\\\\Users\\\\sam\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list4=[]\n",
    "for s in token_list3:\n",
    "    if s not in stopwords.words(\"english\"):\n",
    "        token_list4.append(s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usually',\n",
       " 'corpus',\n",
       " 'break',\n",
       " 'text',\n",
       " 'words',\n",
       " 'symbols',\n",
       " 'sentences',\n",
       " 'paragraphs',\n",
       " 'meaningful',\n",
       " 'elements',\n",
       " 'must',\n",
       " 'done',\n",
       " 'future',\n",
       " 'clearing',\n",
       " 'text',\n",
       " 'without',\n",
       " 'properly',\n",
       " 'clean',\n",
       " 'text',\n",
       " 'like',\n",
       " 'punctuation',\n",
       " 'stop',\n",
       " 'words',\n",
       " 'e.t.c']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_list4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see what are stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopw # these are stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step-5 Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization of the words\n",
    "It is an important step in the text preprocessing. It produces the root word that is generated from it. For example developed, developing have the root words that is “develop”. It is the lemmatized version of the word developed and developing. Lemmatization uses the dictionary to match each word with the root words.\n",
    "\n",
    "For the lemmatization, you have to first download the wordnet from the nltk using nltk.download(“wordnet”). After that, you will use the WordNetLemmatizer() for lemmatizing the tokenized list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "may be lemmatization removes last . (dots) or something in last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_list4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e4b7cdf72c04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtoken_list5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_list4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_list5\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Total tokens : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_list5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'token_list4' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]\n",
    "print(token_list5[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for n in token_list4:\n",
    " #   if n not in lemmatizer.lemmatize(word):\n",
    "  #      token_list5.append(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP-6 Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tools shorten words back to their root form. Stemming is a little more aggressive. It cuts off prefixes and/or endings of words based on common ones. It can sometimes be helpful, but not always because often times the new word is so much a root that it loses its actual meaning. Lemmatizing, on the other hand, maps common words into one base. Unlike stemming though, it always still returns a proper word that can be found in the dictionary. I like to compare the two to see which one works better for what I need. I usually prefer Lemmatizer, but surprisingly, this time, Stemming seemed to have more of an affect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list6=[]\n",
    "for word in token_list5:\n",
    "    stem_word = stemmer.stem(word) \n",
    "    token_list6.append(stem_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see stemming deleted last alphabet of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

Textual data cleaning and preProcessing
we must clean and pre process our data before fitting it to model ... must for NLP(Natural Language Processing) or Data science. so we will do with out favourate language PYTHON.

in this we will learn Tokenization

we will use Natural Language Tool Kit (nltk), so lets install and import it in out program and most important download

pip install nltk
In [1]:
import nltk
nltk.download('punkt')
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\sam\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
Out[1]:
True
step1-Tokenization
For tokenization, nltk has a method word_tokenize(). It will break down the raw text and return as a list. You can also check the difference of future cleaning using the size of the token list.
The first thing we must do to our data is to tokenize it. That is, break up paragraphs into sentences (sentence tokenization) or break up sentences into single words (word tokenization).

For sentence tokenization, use the sent_tokenize method. Most tweets are only one sentence long, so there may not be much of a difference in the tokenization here.

lets try it with some text

In [2]:
raw_text="YOU ARE usually do it in a CORPUS to break down the text into words,symbols, sentences, paragraphs, and other meaningful elements. It must be done for the future clearing of the text. Without it you can not properly clean the text like punctuation, stop words e.t.c."
In [3]:
token_list = nltk.word_tokenize(raw_text) # making a var or pointer
converted to list

In [4]:
token_list
Out[4]:
['YOU',
 'ARE',
 'usually',
 'do',
 'it',
 'in',
 'a',
 'CORPUS',
 'to',
 'break',
 'down',
 'the',
 'text',
 'into',
 'words',
 ',',
 'symbols',
 ',',
 'sentences',
 ',',
 'paragraphs',
 ',',
 'and',
 'other',
 'meaningful',
 'elements',
 '.',
 'It',
 'must',
 'be',
 'done',
 'for',
 'the',
 'future',
 'clearing',
 'of',
 'the',
 'text',
 '.',
 'Without',
 'it',
 'you',
 'can',
 'not',
 'properly',
 'clean',
 'the',
 'text',
 'like',
 'punctuation',
 ',',
 'stop',
 'words',
 'e.t.c',
 '.']
In [5]:
token_list[0:5]
Out[5]:
['YOU', 'ARE', 'usually', 'do', 'it']
lets see total amount of tokens

In [6]:
print("Total tokens : ", len(token_list))
Total tokens :  55
now we can lower case all upper case words
step-2 all upper case are converted to lower case
In [7]:
token_list2=[]
for word in token_list:
    token_list2.append(word.lower())
In [8]:
token_list2
Out[8]:
['you',
 'are',
 'usually',
 'do',
 'it',
 'in',
 'a',
 'corpus',
 'to',
 'break',
 'down',
 'the',
 'text',
 'into',
 'words',
 ',',
 'symbols',
 ',',
 'sentences',
 ',',
 'paragraphs',
 ',',
 'and',
 'other',
 'meaningful',
 'elements',
 '.',
 'it',
 'must',
 'be',
 'done',
 'for',
 'the',
 'future',
 'clearing',
 'of',
 'the',
 'text',
 '.',
 'without',
 'it',
 'you',
 'can',
 'not',
 'properly',
 'clean',
 'the',
 'text',
 'like',
 'punctuation',
 ',',
 'stop',
 'words',
 'e.t.c',
 '.']
In [9]:
print("Total tokens : ", len(token_list2))
Total tokens :  55
step-3 Remove punctuation
In [10]:
from nltk.tokenize import punkt
token_list3 = list(filter(lambda token : punkt.PunktToken(token).is_non_punct,token_list2))
print(token_list3[0:20],"\n")
print("Total tokens : ", len(token_list3))
['you', 'are', 'usually', 'do', 'it', 'in', 'a', 'corpus', 'to', 'break', 'down', 'the', 'text', 'into', 'words', 'symbols', 'sentences', 'paragraphs', 'and', 'other'] 

Total tokens :  47
another method by me
In [11]:
punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
token_list3=[]
for t in token_list2:
    if t not in punctuations:
        token_list3.append(t)
In [12]:
token_list3
Out[12]:
['you',
 'are',
 'usually',
 'do',
 'it',
 'in',
 'a',
 'corpus',
 'to',
 'break',
 'down',
 'the',
 'text',
 'into',
 'words',
 'symbols',
 'sentences',
 'paragraphs',
 'and',
 'other',
 'meaningful',
 'elements',
 'it',
 'must',
 'be',
 'done',
 'for',
 'the',
 'future',
 'clearing',
 'of',
 'the',
 'text',
 'without',
 'it',
 'you',
 'can',
 'not',
 'properly',
 'clean',
 'the',
 'text',
 'like',
 'punctuation',
 'stop',
 'words',
 'e.t.c']
In [13]:
print("Total tokens : ", len(token_list3))
Total tokens :  47
step-4 Removing StopWords
Stop words does not contribute to the text analysis as they don’t have any meaning. Example of the stop words are like in, the, and which e.t.c. It’s better that you should remove from them. Nltk has already the list of the stop words you can use them to compare your tokenize words. You can download the stop words using nltk.download(“stopwords”)

In [14]:
from nltk.corpus import stopwords
In [15]:
nltk.download("stopwords")
[nltk_data] Downloading package stopwords to
[nltk_data]     C:\Users\sam\AppData\Roaming\nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Out[15]:
True
In [16]:
stopwords
Out[16]:
<WordListCorpusReader in 'C:\\Users\\sam\\AppData\\Roaming\\nltk_data\\corpora\\stopwords'>
In [17]:
token_list4=[]
for s in token_list3:
    if s not in stopwords.words("english"):
        token_list4.append(s)
In [18]:
token_list4
Out[18]:
['usually',
 'corpus',
 'break',
 'text',
 'words',
 'symbols',
 'sentences',
 'paragraphs',
 'meaningful',
 'elements',
 'must',
 'done',
 'future',
 'clearing',
 'text',
 'without',
 'properly',
 'clean',
 'text',
 'like',
 'punctuation',
 'stop',
 'words',
 'e.t.c']
In [19]:
len(token_list4)
Out[19]:
24
lets see what are stop words

In [20]:
stopw=stopwords.words("english")
In [21]:
stopw # these are stop words
Out[21]:
['i',
 'me',
 'my',
 'myself',
 'we',
 'our',
 'ours',
 'ourselves',
 'you',
 "you're",
 "you've",
 "you'll",
 "you'd",
 'your',
 'yours',
 'yourself',
 'yourselves',
 'he',
 'him',
 'his',
 'himself',
 'she',
 "she's",
 'her',
 'hers',
 'herself',
 'it',
 "it's",
 'its',
 'itself',
 'they',
 'them',
 'their',
 'theirs',
 'themselves',
 'what',
 'which',
 'who',
 'whom',
 'this',
 'that',
 "that'll",
 'these',
 'those',
 'am',
 'is',
 'are',
 'was',
 'were',
 'be',
 'been',
 'being',
 'have',
 'has',
 'had',
 'having',
 'do',
 'does',
 'did',
 'doing',
 'a',
 'an',
 'the',
 'and',
 'but',
 'if',
 'or',
 'because',
 'as',
 'until',
 'while',
 'of',
 'at',
 'by',
 'for',
 'with',
 'about',
 'against',
 'between',
 'into',
 'through',
 'during',
 'before',
 'after',
 'above',
 'below',
 'to',
 'from',
 'up',
 'down',
 'in',
 'out',
 'on',
 'off',
 'over',
 'under',
 'again',
 'further',
 'then',
 'once',
 'here',
 'there',
 'when',
 'where',
 'why',
 'how',
 'all',
 'any',
 'both',
 'each',
 'few',
 'more',
 'most',
 'other',
 'some',
 'such',
 'no',
 'nor',
 'not',
 'only',
 'own',
 'same',
 'so',
 'than',
 'too',
 'very',
 's',
 't',
 'can',
 'will',
 'just',
 'don',
 "don't",
 'should',
 "should've",
 'now',
 'd',
 'll',
 'm',
 'o',
 're',
 've',
 'y',
 'ain',
 'aren',
 "aren't",
 'couldn',
 "couldn't",
 'didn',
 "didn't",
 'doesn',
 "doesn't",
 'hadn',
 "hadn't",
 'hasn',
 "hasn't",
 'haven',
 "haven't",
 'isn',
 "isn't",
 'ma',
 'mightn',
 "mightn't",
 'mustn',
 "mustn't",
 'needn',
 "needn't",
 'shan',
 "shan't",
 'shouldn',
 "shouldn't",
 'wasn',
 "wasn't",
 'weren',
 "weren't",
 'won',
 "won't",
 'wouldn',
 "wouldn't"]


step-5 Lemmatization
Lemmatization of the words It is an important step in the text preprocessing. It produces the root word that is generated from it. For example developed, developing have the root words that is “develop”. It is the lemmatized version of the word developed and developing. Lemmatization uses the dictionary to match each word with the root words.

For the lemmatization, you have to first download the wordnet from the nltk using nltk.download(“wordnet”). After that, you will use the WordNetLemmatizer() for lemmatizing the tokenized list.

In [22]:
nltk.download('wordnet')
[nltk_data] Downloading package wordnet to
[nltk_data]     C:\Users\sam\AppData\Roaming\nltk_data...
[nltk_data]   Package wordnet is already up-to-date!
Out[22]:
True
may be lemmatization removes last . (dots) or something in last

In [1]:
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]
print(token_list5[0:20],"\n")
print("Total tokens : ", len(token_list5))
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
<ipython-input-1-e4b7cdf72c04> in <module>
      1 from nltk.stem import WordNetLemmatizer
      2 lemmatizer = WordNetLemmatizer()
----> 3 token_list5 = [lemmatizer.lemmatize(word) for word in token_list4]
      4 print(token_list5[0:20],"\n")
      5 print("Total tokens : ", len(token_list5))

NameError: name 'token_list4' is not defined
In [ ]:
#for n in token_list4:
 #   if n not in lemmatizer.lemmatize(word):
  #      token_list5.append(n)
STEP-6 Stemming
Both tools shorten words back to their root form. Stemming is a little more aggressive. It cuts off prefixes and/or endings of words based on common ones. It can sometimes be helpful, but not always because often times the new word is so much a root that it loses its actual meaning. Lemmatizing, on the other hand, maps common words into one base. Unlike stemming though, it always still returns a proper word that can be found in the dictionary. I like to compare the two to see which one works better for what I need. I usually prefer Lemmatizer, but surprisingly, this time, Stemming seemed to have more of an affect.

In [ ]:
from nltk.stem.porter import PorterStemmer
In [ ]:
stemmer = PorterStemmer()
In [ ]:
token_list6=[]
for word in token_list5:
    stem_word = stemmer.stem(word) 
    token_list6.append(stem_word)
see stemming deleted last alphabet of words

In [ ]:
token_list6
